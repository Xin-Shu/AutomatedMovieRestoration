# -*- coding: utf-8 -*-
"""MessyCode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H-HrA7boOhFw96suR-J5Y11wJcg4jSxP
"""

import os
import PIL
import random
import numpy as np
from PIL import ImageOps
from IPython.display import Image, display
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import load_img
# import tensorflow_addons as tfa

# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['DML_VISIBLE_DEVICES'] = '0'
# tfv1.debugging.set_log_device_placement(True)
# tfv1.enable_eager_execution()

input_dir = "M:/MAI_dataset/tempSamples/degraded/"
target_dir = "M:/MAI_dataset/tempSamples/mask/"
img_size = (180, 320)  # (273, 640)(180, 320)
num_classes = 2
batch_size = 2

input_img_paths = sorted(
    [
        os.path.join(input_dir, fname)
        for fname in os.listdir(input_dir)
        if fname.endswith(".png")
    ]
)
target_img_paths = sorted(
    [
        os.path.join(target_dir, fname)
        for fname in os.listdir(target_dir)
        if fname.endswith(".png") and not fname.startswith(".")
    ]
)

print("Number of samples:", len(input_img_paths))


class GeneralDetection(keras.utils.Sequence):
    """Helper to iterate over the data (as Numpy arrays)."""

    def __init__(self, batch_size_, img_size_, input_img_paths_, target_img_paths_):
        self.batch_size = batch_size_
        self.img_size = img_size_
        self.input_img_paths = input_img_paths_
        self.target_img_paths = target_img_paths_

    def __len__(self):
        return len(self.target_img_paths) // self.batch_size

    def __getitem__(self, idx):
        """Returns tuple (input, target) correspond to batch #idx."""
        i = idx * self.batch_size
        batch_input_img_paths = self.input_img_paths[i:i + self.batch_size]
        batch_target_img_paths = self.target_img_paths[i:i + self.batch_size]
        x = np.zeros((self.batch_size,) + self.img_size + (1,), dtype="float32")  # uint8
        for j, path in enumerate(batch_input_img_paths):
            img = load_img(path, target_size=self.img_size, color_mode="grayscale")
            x[j] = np.expand_dims(img, 2)
        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype="float32")
        for j, path in enumerate(batch_target_img_paths):
            img = load_img(path, target_size=self.img_size, color_mode="grayscale")
            y[j] = np.expand_dims(img, 2)
        # print(x[300, 100], y[300, 100])
        return x, y


def get_model(img_size_, num_classes_):
    inputs = keras.Input(shape=img_size_ + (1,))  # img_size_ = [180, 320, 1]

    """ [First half of the network: downsampling inputs] """

    # Entry block
    x1 = layers.Conv2D(32, 3, padding="same", activation="relu")(inputs)
    x1 = layers.BatchNormalization()(x1)

    x2 = layers.Conv2D(64, 2, padding="same", activation="relu")(x1)
    x2 = layers.BatchNormalization()(x2)

    x3 = layers.Conv2D(128, 2, padding="same", activation="relu")(x2)
    x3 = layers.BatchNormalization()(x3)

    encoder = layers.concatenate([x1, x2, x3])
    encoder = layers.Conv2D(64, 1, padding="same")(encoder)
    encoder = layers.MaxPooling2D(pool_size=(2, 2))(encoder)

    y1 = layers.UpSampling2D(2)(encoder)
    y1 = layers.Conv2DTranspose(32, 3, padding="same", activation="relu")(y1)
    y1 = layers.BatchNormalization()(y1)

    y2 = layers.Conv2DTranspose(32, 3, padding="same", activation="relu")(y1)
    y2 = layers.BatchNormalization()(y2)

    # Add a per-pixel classification layer
    outputs = layers.Conv2D(num_classes_, 3, activation="softmax", padding="same")(y2)

    # Define the model
    model_ = keras.Model(inputs, outputs)
    return model_


# Split our img paths into a training and a validation set
val_samples = int(0.5 * len(input_img_paths))
# random.Random(1337).shuffle(input_img_paths)
# random.Random(1337).shuffle(target_img_paths)
train_input_img_paths = input_img_paths[:-val_samples]
train_target_img_paths = target_img_paths[:-val_samples]
val_input_img_paths = input_img_paths[-val_samples:]
val_target_img_paths = target_img_paths[-val_samples:]

# Instantiate data Sequences for each split
train_gen = GeneralDetection(batch_size, img_size, train_input_img_paths, train_target_img_paths)
val_gen = GeneralDetection(batch_size, img_size, val_input_img_paths, val_target_img_paths)

keras.backend.clear_session()

use_pretrained = False
if use_pretrained:
    model = keras.models.load_model("generalDegradedDetection.h5")
else:
    # Build model
    model = get_model(img_size, num_classes)
    model.summary()
    optimizer = tf.contrib.opt.AdamWOptimizer(learning_rate=0.01, weight_decay=0.0001)
    model.compile(optimizer=optimizer,
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                  metrics=["accuracy"]
                  )
    callbacks = [
        keras.callbacks.ModelCheckpoint("generalDegradedDetection.h5", save_best_only=True)
    ]
    epochs = 20
    history = model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)

    # list all data in history
    print(history.history.keys())
    # summarize history for accuracy
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')

    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')


i = 100
val_gen = GeneralDetection(batch_size, img_size, val_input_img_paths, val_target_img_paths)
val_preds = model.predict(val_gen[i])


def display_mask():
    """Quick utility to display a model's prediction."""
    mask = np.argmax(val_preds, axis=-1)
    mask = np.expand_dims(mask, axis=-1)
    a_file = open("temp/test.txt", "w")
    arr_reshaped = mask.reshape(mask.shape[0], -1)
    np.savetxt(a_file, arr_reshaped)
    a_file.close()


# Display results for validation image #10


# Display input image
# img1 = Image(filename=val_input_img_paths[i])
print(f'Length: {len(val_input_img_paths)}')
print(val_input_img_paths[i])
# Display ground-truth target mask
img2 = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))

# Display mask predicted by our model
display_mask()  # Note that the model only sees inputs at 150x150.
img2.save('temp/original.png')
plt.show()
keras.backend.clear_session()
